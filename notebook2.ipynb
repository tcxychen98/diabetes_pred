{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d71374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CatBoost if missing\n",
    "# !pip install catboost scikit-learn pandas numpy\n",
    "\n",
    "import catboost\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"CatBoost Version: {catboost.__version__}\")\n",
    "print(f\"Scikit-learn Version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd58f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Running Seed: 42\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 16 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 17 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 16 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/tcxychen/Documents/GitHub/diabetes_pred/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:396: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 17 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ============================================\n",
    "# 1. CONFIGURATION\n",
    "# ============================================\n",
    "SEEDS = [42, 123, 777]\n",
    "N_FOLDS = 10\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "# Fixed CatBoost Params\n",
    "CATBOOST_PARAMS = {\n",
    "    'iterations': 2000,           # High number, let early_stopping cut it\n",
    "    'learning_rate': 0.02,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_seed': 42,            # Will be overwritten by loop seed\n",
    "    'allow_writing_files': False, # Keep directory clean\n",
    "    'thread_count': -1,           # Use all CPU cores\n",
    "    'task_type': 'CPU'            # Strict determinism\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 2. DATA PREPARATION\n",
    "# ============================================\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Helper: Basic Feature Engineering (The robust ones we found earlier)\n",
    "def base_feature_eng(df):\n",
    "    df_new = df.copy()\n",
    "    df_new['TG_HDL_Ratio'] = df_new['triglycerides'] / (df_new['hdl_cholesterol'] + 1)\n",
    "    df_new['MAP'] = (df_new['systolic_bp'] + 2 * df_new['diastolic_bp']) / 3\n",
    "    df_new['BMI_Age'] = df_new['bmi'] * df_new['age']\n",
    "    return df_new\n",
    "\n",
    "# Apply Base Engineering\n",
    "X = base_feature_eng(train_df.drop(['diagnosed_diabetes', 'id'], axis=1))\n",
    "y = train_df['diagnosed_diabetes']\n",
    "X_test_base = base_feature_eng(test_df.drop(['id'], axis=1))\n",
    "\n",
    "# Identify Columns\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# ============================================\n",
    "# 3. TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "# Storage for predictions\n",
    "oof_preds = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(X_test_base))\n",
    "\n",
    "# Average over seeds\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n{'='*20}\\nRunning Seed: {seed}\\n{'='*20}\")\n",
    "    \n",
    "    # Update seed in params\n",
    "    CATBOOST_PARAMS['random_seed'] = seed\n",
    "    \n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    \n",
    "    seed_test_preds = np.zeros(len(X_test_base))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        # 1. Split Data\n",
    "        X_train, y_train = X.iloc[train_idx].copy(), y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx].copy(), y.iloc[val_idx]\n",
    "        \n",
    "        # 2. IN-FOLD BINNING (The Critical Step)\n",
    "        # We fit the binner ONLY on X_train to avoid leakage\n",
    "        # Strategy: 'quantile' (Statistical binning)\n",
    "        binner = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile', subsample=None)\n",
    "        \n",
    "        # Fit on Train, Transform Train\n",
    "        # We create NEW columns for the bins (preserving original raw data usually helps CatBoost)\n",
    "        # But if the prompt implies \"using binning methods\", we can append them as categorical features\n",
    "        \n",
    "        train_binned = binner.fit_transform(X_train[num_cols])\n",
    "        val_binned = binner.transform(X_val[num_cols])\n",
    "        test_binned = binner.transform(X_test_base[num_cols])\n",
    "        \n",
    "        # Create DataFrames for the binned features\n",
    "        binned_cols = [f\"{c}_bin\" for c in num_cols]\n",
    "        X_train_bins = pd.DataFrame(train_binned, columns=binned_cols, index=X_train.index)\n",
    "        X_val_bins = pd.DataFrame(val_binned, columns=binned_cols, index=X_val.index)\n",
    "        X_test_bins = pd.DataFrame(test_binned, columns=binned_cols, index=X_test_base.index)\n",
    "        \n",
    "        # Concatenate: Raw Features + Binned Features\n",
    "        X_train_final = pd.concat([X_train, X_train_bins], axis=1)\n",
    "        X_val_final = pd.concat([X_val, X_val_bins], axis=1)\n",
    "        X_test_final = pd.concat([X_test_base, X_test_bins], axis=1)\n",
    "        \n",
    "        # IMPORTANT: CatBoost needs to know which cols are categorical\n",
    "        # The new binned cols are effectively ordinal/categorical\n",
    "        # We cast them to int for CatBoost\n",
    "        X_train_final[binned_cols] = X_train_final[binned_cols].astype(int)\n",
    "        X_val_final[binned_cols] = X_val_final[binned_cols].astype(int)\n",
    "        X_test_final[binned_cols] = X_test_final[binned_cols].astype(int)\n",
    "        \n",
    "        # Full list of categorical features (Original Cats + New Bins)\n",
    "        full_cat_features = cat_cols + binned_cols\n",
    "        \n",
    "        # 3. Create CatBoost Pools\n",
    "        train_pool = Pool(X_train_final, y_train, cat_features=full_cat_features)\n",
    "        val_pool = Pool(X_val_final, y_val, cat_features=full_cat_features)\n",
    "        test_pool = Pool(X_test_final, cat_features=full_cat_features)\n",
    "        \n",
    "        # 4. Train\n",
    "        model = CatBoostClassifier(**CATBOOST_PARAMS)\n",
    "        model.fit(\n",
    "            train_pool,\n",
    "            eval_set=val_pool,\n",
    "            early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "            use_best_model=True,\n",
    "            verbose=False # Silence fold output to keep notebook clean\n",
    "        )\n",
    "        \n",
    "        # 5. Predict\n",
    "        val_probs = model.predict_proba(val_pool)[:, 1]\n",
    "        seed_test_preds += model.predict_proba(test_pool)[:, 1] / N_FOLDS\n",
    "        \n",
    "        # Accumulate OOF (Divide by len(SEEDS) later)\n",
    "        oof_preds[val_idx] += val_probs / len(SEEDS)\n",
    "        \n",
    "        # Optional: Print Fold Score\n",
    "        # score = roc_auc_score(y_val, val_probs)\n",
    "        # print(f\"  Fold {fold+1} AUC: {score:.4f}\")\n",
    "        \n",
    "    # Add seed predictions to global test preds\n",
    "    test_preds += seed_test_preds / len(SEEDS)\n",
    "    \n",
    "    # Quick check for this seed\n",
    "    # (Note: This is an approximation since oof_preds is being built incrementally)\n",
    "    print(\"  Seed completed.\")\n",
    "\n",
    "# ============================================\n",
    "# 4. EVALUATION & SAVING\n",
    "# ============================================\n",
    "\n",
    "# Calculate Final OOF Score\n",
    "final_auc = roc_auc_score(y, oof_preds)\n",
    "print(f\"\\nFinal OOF ROC-AUC Score: {final_auc:.5f}\")\n",
    "\n",
    "# Save OOF File (For local comparison)\n",
    "oof_df = pd.DataFrame({'id': train_df['id'], 'diagnosed_diabetes': y, 'prediction': oof_preds})\n",
    "oof_df.to_csv('catboost_final_oof.csv', index=False)\n",
    "print(\"Saved OOF predictions to 'catboost_final_oof.csv'\")\n",
    "\n",
    "# Save Submission\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'diagnosed_diabetes': test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved final submission to 'submission.csv'\")\n",
    "display(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
